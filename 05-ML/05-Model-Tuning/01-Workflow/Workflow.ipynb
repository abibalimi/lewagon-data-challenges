{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow & Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Import the house price data set. We will only keep numerical feature for sake of simplicity\n",
    "\n",
    "Your goal will be to fit the best KNN Regressor. And in particular, how many \"neighbors\" (K in KNN) should you consider to best predict your house-price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>119.0</td>\n",
       "      <td>790</td>\n",
       "      <td>...</td>\n",
       "      <td>349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>830</td>\n",
       "      <td>...</td>\n",
       "      <td>736</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1121 rows √ó 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\n",
       "0        1          60         65.0     8450            7            5   \n",
       "1        2          20         80.0     9600            6            8   \n",
       "2        3          60         68.0    11250            7            5   \n",
       "3        4          70         60.0     9550            7            5   \n",
       "4        5          60         84.0    14260            8            5   \n",
       "...    ...         ...          ...      ...          ...          ...   \n",
       "1455  1456          60         62.0     7917            6            5   \n",
       "1456  1457          20         85.0    13175            6            6   \n",
       "1457  1458          70         66.0     9042            7            9   \n",
       "1458  1459          20         68.0     9717            5            6   \n",
       "1459  1460          20         75.0     9937            5            6   \n",
       "\n",
       "      YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  WoodDeckSF  \\\n",
       "0          2003          2003       196.0         706  ...           0   \n",
       "1          1976          1976         0.0         978  ...         298   \n",
       "2          2001          2002       162.0         486  ...           0   \n",
       "3          1915          1970         0.0         216  ...           0   \n",
       "4          2000          2000       350.0         655  ...         192   \n",
       "...         ...           ...         ...         ...  ...         ...   \n",
       "1455       1999          2000         0.0           0  ...           0   \n",
       "1456       1978          1988       119.0         790  ...         349   \n",
       "1457       1941          2006         0.0         275  ...           0   \n",
       "1458       1950          1996         0.0          49  ...         366   \n",
       "1459       1965          1965         0.0         830  ...         736   \n",
       "\n",
       "      OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n",
       "0              61              0          0            0         0        0   \n",
       "1               0              0          0            0         0        0   \n",
       "2              42              0          0            0         0        0   \n",
       "3              35            272          0            0         0        0   \n",
       "4              84              0          0            0         0        0   \n",
       "...           ...            ...        ...          ...       ...      ...   \n",
       "1455           40              0          0            0         0        0   \n",
       "1456            0              0          0            0         0        0   \n",
       "1457           60              0          0            0         0     2500   \n",
       "1458            0            112          0            0         0        0   \n",
       "1459           68              0          0            0         0        0   \n",
       "\n",
       "      MoSold  YrSold  SalePrice  \n",
       "0          2    2008     208500  \n",
       "1          5    2007     181500  \n",
       "2          9    2008     223500  \n",
       "3          2    2006     140000  \n",
       "4         12    2008     250000  \n",
       "...      ...     ...        ...  \n",
       "1455       8    2007     175000  \n",
       "1456       2    2010     210000  \n",
       "1457       5    2010     266500  \n",
       "1458       4    2010     142125  \n",
       "1459       6    2008     147500  \n",
       "\n",
       "[1121 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data\n",
    "data = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/houses_train_raw.csv')\n",
    "\n",
    "# Only keep numerical columns and raws without NaN\n",
    "data = data.select_dtypes(include=np.number).dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['SalePrice'])\n",
    "y = data['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Split the data to create your `X_train` `X_test` and `y_train` `y_test`. Use:\n",
    "- `test_size=0.3`\n",
    "- `random_state=0` to compare with your buddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is always very important for KNN.\n",
    "\n",
    "‚ùì _Standard-Scale_ your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.375112</td>\n",
       "      <td>-0.878719</td>\n",
       "      <td>0.186885</td>\n",
       "      <td>-0.066635</td>\n",
       "      <td>-0.159838</td>\n",
       "      <td>1.357895</td>\n",
       "      <td>-0.445886</td>\n",
       "      <td>-1.314462</td>\n",
       "      <td>0.108204</td>\n",
       "      <td>0.326950</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.151130</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>-0.749484</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>2.195993</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>-0.110711</td>\n",
       "      <td>0.168285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.309121</td>\n",
       "      <td>-0.878719</td>\n",
       "      <td>2.108009</td>\n",
       "      <td>0.117185</td>\n",
       "      <td>-0.871846</td>\n",
       "      <td>0.415780</td>\n",
       "      <td>0.093483</td>\n",
       "      <td>-0.507574</td>\n",
       "      <td>-0.584157</td>\n",
       "      <td>0.659783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352925</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>-0.749484</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>-0.277928</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>-1.965356</td>\n",
       "      <td>-0.577113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.576332</td>\n",
       "      <td>-0.878719</td>\n",
       "      <td>-0.453490</td>\n",
       "      <td>-0.533191</td>\n",
       "      <td>-0.871846</td>\n",
       "      <td>2.300009</td>\n",
       "      <td>-0.699707</td>\n",
       "      <td>0.773954</td>\n",
       "      <td>-0.584157</td>\n",
       "      <td>-0.065154</td>\n",
       "      <td>...</td>\n",
       "      <td>1.117452</td>\n",
       "      <td>0.830288</td>\n",
       "      <td>-0.749484</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>-0.277928</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>0.260218</td>\n",
       "      <td>-0.577113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -1.375112 -0.878719  0.186885 -0.066635 -0.159838  1.357895 -0.445886   \n",
       "1 -1.309121 -0.878719  2.108009  0.117185 -0.871846  0.415780  0.093483   \n",
       "2  0.576332 -0.878719 -0.453490 -0.533191 -0.871846  2.300009 -0.699707   \n",
       "\n",
       "         7         8         9   ...        27        28        29        30  \\\n",
       "0 -1.314462  0.108204  0.326950  ... -1.151130 -0.738939 -0.749484 -0.372566   \n",
       "1 -0.507574 -0.584157  0.659783  ... -0.352925 -0.738939 -0.749484 -0.372566   \n",
       "2  0.773954 -0.584157 -0.065154  ...  1.117452  0.830288 -0.749484 -0.372566   \n",
       "\n",
       "         31        32        33        34        35        36  \n",
       "0 -0.109954  2.195993 -0.061673 -0.146085 -0.110711  0.168285  \n",
       "1 -0.109954 -0.277928 -0.061673 -0.146085 -1.965356 -0.577113  \n",
       "2 -0.109954 -0.277928 -0.061673 -0.146085  0.260218 -0.577113  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaled_X = scaler.transform(X_train)\n",
    "pd.DataFrame(scaled_X).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì 5-fold cross validate a simple KNN regressor taking into account only the closest neighbor, and compute its mean cv-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=1)\n",
    "#knn_model.fit(scaled_X, y_train)\n",
    "#knn_model.score(scaled_X, y_train)#, knn_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== > NO FITTING IS NEEDED AT THIS POINT :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.569025195507008"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_knn = cross_validate(knn_model, scaled_X, y_train)\n",
    "cv_knn['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid search\n",
    "\n",
    "Let's use sklearn `GridSearchCV` to find the best KNN hyperparameter `n_neighbors`.\n",
    "- Start coarse-grain approach, with `n_neighbors` = [1,5,10,20,50]\n",
    "- 5-fold cross validate each combination\n",
    "- Be sure to maximize your performance time using `n_jobs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'n_neighbors' : [1,5,10,20,50]}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = GridSearchCV(knn_model, grid, \n",
    "                           scoring = 'r2',\n",
    "                           cv = 10,\n",
    "                           n_jobs=-1 # paralellize computation\n",
    "                          ) \n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(scaled_X,y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì According to the grid search, what is the optimal K value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 20}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What is the best score the optimal K value produced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7685936824413232"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an idea about where the best k lies, but some of the values we did not try could be better!\n",
    "\n",
    "Re-run a fine-grain grid search with k values around to your previous best value\n",
    "\n",
    "‚ùì What is the `best_score` and `best_k` you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'n_neighbors' : [11,12,13,14,15,16,17,18,19]}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = GridSearchCV(knn_model, grid, \n",
    "                           scoring = 'r2',\n",
    "                           cv = 10,\n",
    "                           n_jobs=-1 # paralellize computation\n",
    "                          ) \n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(scaled_X,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_neighbors': 16}, 0.7724899314806318)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_, search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 0.7724899314806318)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score = search.best_score_\n",
    "best_k = search.best_params_['n_neighbors']\n",
    "best_k, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.8.6, pytest-6.2.3, py-1.10.0, pluggy-0.13.1 -- /Users/smrack/.pyenv/versions/3.8.6/envs/lewagon/bin/python3.8\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/smrack/code/olushO/data-challenges/05-ML/05-Model-Tuning/01-Workflow\n",
      "plugins: anyio-2.2.0, dash-1.20.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/test_knn.py::TestKnn::test_best_k \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 50%]\u001b[0m\n",
      "tests/test_knn.py::TestKnn::test_best_score \u001b[32mPASSED\u001b[0m\u001b[32m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.44s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/knn.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed knn step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('knn',\n",
    "                         best_k=best_k,\n",
    "                         best_score=best_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è This problem is actually simple enough to perform a grid search manually.\n",
    "- Loop manually over all values of k from 1 to 50 and store the mean cv-scores of each model in a list.\n",
    "- Plot the score as a function of k to visualy find the best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores = []\n",
    "for k in range(1,51):\n",
    "    model = KNeighborsRegressor(n_neighbors=k)\n",
    "    cv = cross_validate(model, scaled_X, y_train, cv=10)\n",
    "    knn_scores.append(cv['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1,51), knn_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(knn_scores)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7724899314806318"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(knn_scores).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The possibility to paralellize makes GridSearchCV better a better option than manual loop !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCan you guess what makes GridSearchCV a better option than such manual loop ?\n",
    " \n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "- Sklearn's `n_jobs=-1` allows you to paralellize search of each CPU\n",
    "- What if you had multiple hyper-parameters to co-optimize ?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNNRegressor suppports various _distance metrics_ via the hyper-parameter `p` [see docs](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "‚ùìUse GridSearchCV to search for best `k` and `p` at the same time: Try all combinations for `k` = [1, 5, 10, 20, 50] and `p` = [1, 2, 3]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'n_neighbors' : [1,5,10,20,50],\n",
    "        'p': [1,2,3]}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = GridSearchCV(knn_model, grid, \n",
    "                           scoring = 'r2',\n",
    "                           cv = 5,\n",
    "                           n_jobs=-1 # paralellize computation\n",
    "                          ) \n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(scaled_X,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì How many models did you trained overall?\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "\n",
    "Much more than 15. Think twice :)\n",
    "    <details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "75 models due to CV=5\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolutely :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What are the best parameters and the best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 10, 'p': 1}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7978142226309175"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=10, p=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Search\n",
    "\n",
    "Now let's see if a Random Search can find a better combination with the same number of model fits?\n",
    "Use `RandomizedSearchCV` to\n",
    "- Randomly sample `k` from a uniform `randint(1,50)` distribition\n",
    "- Sample `p` from a list [1,2,3]\n",
    "- Use the correct number of `n_iter` and `cv` to fit the exact same number of models than in your previous GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'n_neighbors': stats.randint(10, 31),\n",
    "        'p': [1,2,3]}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = RandomizedSearchCV(knn_model, \n",
    "                            grid, \n",
    "                            scoring = 'r2',\n",
    "                            n_iter= 5,\n",
    "                            cv = 5,\n",
    "                            n_jobs=-1 # paralellize computation\n",
    "                           ) \n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(scaled_X,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=15, p=1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá This is your final chance to fine-tune your model\n",
    "- Refine your RandomsearchCV if you wish\n",
    "- Choose your best model hyper-params and instantiate it\n",
    "- Re-fit it on the __entire__ train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=16, p=1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsRegressor(n_neighbors=16, p=1)\n",
    "knn_model.fit(scaled_X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Time has come to discover our model's performance on the **unseen** test set `X_test`. Compute the r2 score for the test set and save it as `r2_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7775524285303221"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_test = knn_model.score(scaler.transform(X_test), y_test)\n",
    "r2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Would you consider the optimized model to generalize well?\n",
    "\n",
    "<details><summary>Answer</summary>\n",
    "\n",
    "Test score may decrease a bit with train set. Probably not more than 5%. This can be due to\n",
    "- An non-representative train/test split\n",
    "- A cross-val number too small leading to overfitting the model-tuning phase. The more you cross-validated, the more robust your findings will generalize - but you can't increase cv too much if your dataset is too small as you won't keep enough observations in each fold to be representative.\n",
    "- Our dataset is very small and our hyperparameter optimization is thus extremely dependent (and overfitting) on our train/test split. Always make sure your dataset is much bigger than the total number of hyperparameter combinations you are trying out!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üß™ Test your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('r2', \n",
    "                         r2_test=r2_test)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Congratulation. Please push the exercise once completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
