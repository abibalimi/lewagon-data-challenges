{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open this notebook with Google Colab ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's load data from the Wine review Kaggle challenge\n",
    "\n",
    "You can download the data from the [dedicated Kaggle page](https://www.kaggle.com/zynicide/wine-reviews).\n",
    "\n",
    "However, as we are running this on Google Colab, let's see how to load the data from Kaggle directly to Google Colab. First, let's install the `kaggle` package : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, go to your Kaggle account and create a \"New API TOKEN\". It will launch the download of a file that you can store on your compute. Now, you have to load this file with the following command : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, just run : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now download the dataset thanks to : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d zynicide/wine-reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now unzip the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('wine-reviews.zip', 'r')\n",
    "zip_ref.extractall('files')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the files that you have downloaded : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now read one of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = 'files'\n",
    "df = pd.read_csv(os.path.join(data_path, 'winemag-data-130k-v2.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line of the dataset corresponds to a wine description. Let's check out what is in the data, in particular the different columns as : \n",
    "- the price of the wine bottle\n",
    "- its description\n",
    "- the number of \"points\" it has (on a scale from 0 to 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is no to do any data engineering. So let's take care of the missing values by removing the corresponding lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0, subset=['price', 'description', 'points'], how='any')\n",
    "\n",
    "df = df[:5000] # Always start like that, not to waste any time first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take input data. Again, the goal is not to do any data engineering so let's skip the train/test splits here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df['description']\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the data, we see that each description is a long list of strings. However, if we don't tell the computer where the words starts and ends, it will consider it as a long and unique word. For that reason, you have to split it into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = X_text.apply(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another step is to convert your words into tokens as it is what the computer will work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_text)\n",
    "X_tokens = tk.texts_to_sequences(X_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to pad the data. But instead of padding it to the maximum length within the input data, let's pad it to a smaller number to accelerate the algorithm convergence - at almost no cost as there are reasons to believe that we don't need the entire sentence to get the importance of the wine and thus its price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(_) for _ in X_tokens])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 40\n",
    "X_pad = pad_sequences(X_tokens, dtype=float, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A sequential model to handle text\n",
    "\n",
    "Let's first build a model with : \n",
    "- an Embedding designed for our task\n",
    "- a Conv1D layer instead of a RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=20000, output_dim=10, mask_zero=True, input_length=maxlen))\n",
    "model.add(layers.Conv1D(10, kernel_size=5, activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer='adam', metrics=['mae'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!> There is another way to check the model layers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(model, \"sequential_model.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's dive into TensorBoard ! This is a amazing tool to see how the Neural Network works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf \n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the logs of our fit will be given to a specifically designed folder that stores some of the information that Tensorboard needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_pad, y.values, \n",
    "          epochs=5, \n",
    "          batch_size=32,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But how to take into account other information?\n",
    "\n",
    "However, how to do to take other information into the model? For example the points (on a 0 to 100 scale) of each bottle, which should tell us how cheap or expensive a wine can be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('price', 'points')\n",
    "plt.xlim(0, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = df['points'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all the models are sequential! In fact, what we will do is have two branches, one for the text and the other for standard data\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/lewagon/data-images/master/DL/stack_layers.png\" width='70%'>\n",
    "\n",
    "The very interesting aspect is that you can optimize the two sub-NN jointly! \n",
    "\n",
    "### How to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a branch for the points values\n",
    "\n",
    "input_num = layers.Input(shape=(1,))\n",
    "\n",
    "x_num = layers.Dense(64, activation=\"relu\")(input_num)\n",
    "x_num = layers.Dense(32, activation=\"relu\")(x_num)\n",
    "x_num = layers.Dense(4, activation=\"relu\")(x_num)\n",
    "x_num = models.Model(inputs=input_num, outputs=x_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the second branch for the text\n",
    "input_text = layers.Input(shape=(maxlen,))\n",
    "\n",
    "x_text = layers.Embedding(input_dim=20000, output_dim=10, mask_zero=True)(input_text)\n",
    "x_text = layers.Conv1D(10, kernel_size=5, activation='relu')(x_text)\n",
    "x_text = layers.Flatten()(x_text)\n",
    "x_text = layers.Dense(10, activation='relu')(x_text)\n",
    "x_text = models.Model(inputs=input_text, outputs=x_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine the two streams of data and add two dense layers on top!\n",
    "combined = layers.concatenate([x_text.output, x_num.output])\n",
    "output = layers.Dense(2, activation=\"relu\")(combined)\n",
    "output = layers.Dense(1, activation=\"linear\")(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here write the entire data flow into the neural network. \n",
    "\n",
    "This line defines the entire model - the previous consecutive layers are not enough as we didn't say that the model is Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_combined = models.Model(inputs=[x_text.input, x_num.input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can combine and fit it!\n",
    "\n",
    "And pay attention to the `X` data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_combined.compile(loss=\"mse\", optimizer='adam', metrics=['mae'])\n",
    "\n",
    "model_combined.fit(x=[X_pad, X_num], \n",
    "          y=y,\n",
    "          epochs=100, \n",
    "          batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the summary now : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(model_combined, \"multi_input_model.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think about each stream / branch as an input source of data. And there are many use-cases where you can encounter such type of data?\n",
    "\n",
    "# Any example of where you might have such data?\n",
    "\n",
    "\n",
    "- Medical data : ECG, EEG, MRI, PET, cognitive assessments, biomarkers, ...\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/lewagon/data-images/master/DL/medical_data.png\" width='70%'>\n",
    "\n",
    "- Object detection, in autonomous car for instance where you take a decision based on many sensors (multiple cameras, radars, speed, map, ...\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/lewagon/data-images/master/DL/autonomous_vehicle.png\" width='70%'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
